{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6eeb8cd6",
   "metadata": {},
   "source": [
    "中文nlpTools推荐：ltp(性能好)和jiagu(功能多),Synonyms(相似度常用)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "departmental-boutique",
   "metadata": {},
   "source": [
    "# 字符串数量统计"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "employed-longitude",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T12:25:32.048868Z",
     "start_time": "2021-04-25T12:25:32.036868Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "统计结果为:\n",
      "{'p': 12, 'n': 13, '<': 8, '*': 10, '?': 12, '@': 10, ';': 10, 'f': 10, 'V': 11, '%': 13, '$': 13, 'm': 12, 'A': 9, 'T': 6, 'x': 7, '!': 7, 'O': 4, 'y': 14, '4': 12, '>': 12, 'l': 7, 'P': 15, 'E': 9, '-': 4, 'j': 9, '\\\\': 12, 'S': 10, '3': 9, '&': 11, 'z': 11, 's': 15, 'U': 6, '5': 17, '6': 10, '\"': 15, 'u': 15, ')': 7, ']': 12, 'b': 5, '|': 8, '9': 13, 'J': 14, 'F': 16, 'q': 12, '{': 8, 'Y': 10, 'C': 13, 'X': 7, ':': 15, '.': 12, 'k': 16, 'o': 5, 'c': 15, '_': 10, 'B': 16, 'M': 6, 'w': 23, 'R': 16, '1': 17, 'd': 15, '0': 12, 'D': 6, 'W': 13, '=': 13, 'a': 9, '~': 14, '7': 8, 'K': 10, 'e': 8, '8': 9, 'L': 5, '[': 9, 'i': 13, 'N': 10, 'r': 12, ',': 7, \"'\": 7, '}': 9, '(': 9, 'h': 13, '`': 13, 'v': 12, 'Q': 7, 'g': 12, 't': 15, 'G': 9, '/': 11, '^': 9, '2': 9, '+': 6, 'Z': 3, '#': 6, 'I': 9, 'H': 12}\n"
     ]
    }
   ],
   "source": [
    "# 下面的代码首先生成包含1000个随机字符的字符串\n",
    "# 然后统计每个字符的出现次数\n",
    "import string\n",
    "import random\n",
    "x = string.ascii_letters + string.digits + string.punctuation\n",
    "# print('\\n可使用的字符集:\\n', len(x))   \n",
    "y = [random.choice(x) for i in range(1000)]\n",
    "# print(y)\n",
    "z = ''.join(y)\n",
    "d = dict()\n",
    "# 遍历生成并且统计数量\n",
    "for ch in z:\n",
    "\td[ch] = d.get(ch, 0) + 1  #千万不要使用d[ch] =d[ch] + 1，否则字典没有ch的key，出错\n",
    "# print('\\n产生字符串为:\\n',z,sep='')\n",
    "print('\\n统计结果为:\\n',d,sep='')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-capital",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 句子相似度计算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "considerable-discipline",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-25T16:28:03.951035Z",
     "start_time": "2021-04-25T16:28:03.941033Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8571428571428571\n",
      "0.3157894736842105\n"
     ]
    }
   ],
   "source": [
    "# 计算句子相似度\n",
    "import jieba\n",
    "\n",
    "from difflib import SequenceMatcher#导入库\n",
    "def similarity(a, b):\n",
    "    return SequenceMatcher(None, a, b).ratio()#引用ratio方法，返回序列相似性的度量\n",
    "print(similarity((6,6,6,6,6,6,6), (6,6,6,6,6,6,7)))\n",
    "\n",
    "def Jaccrad(model, reference):  # terms_reference为源句子，terms_model为候选句子\n",
    "    terms_reference = jieba.cut(reference)  # 默认精准模式\n",
    "    terms_model = jieba.cut(model)\n",
    "    grams_reference = set(terms_reference)  # 去重；如果不需要就改为list\n",
    "    grams_model = set(terms_model)\n",
    "    temp = 0\n",
    "    for i in grams_reference:\n",
    "        if i in grams_model:\n",
    "            temp = temp + 1\n",
    "    fenmu = len(grams_model) + len(grams_reference) - temp  # 并集\n",
    "    jaccard_coefficient = float(temp / fenmu)  # 交集\n",
    "    return jaccard_coefficient\n",
    "\n",
    "\n",
    "a = \"飞控系统主要组成以及交联设备:飞行传感器系统(子系统)的说明是什么？\"\n",
    "b = \"飞行传感器系统(子系统)\"\n",
    "jaccard_coefficient = Jaccrad(a, b)\n",
    "print(jaccard_coefficient)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "jewish-rendering",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# 在句子中找到p然后分割后计算相似度来找s\n",
    "import pymongo\n",
    "client=pymongo.MongoClient('mongodb://localhost:27017')\n",
    "db = client.cndbpedia03\n",
    "collection = db.triples\n",
    "import bson\n",
    "import jieba\n",
    "\n",
    "def Jaccrad(model, reference):  # terms_reference为源句子，terms_model为候选句子,计算句子相似度\n",
    "    terms_reference = jieba.cut(reference)  # 默认精准模式\n",
    "    terms_model = jieba.cut(model)\n",
    "    grams_reference = set(terms_reference)  # 去重；如果不需要就改为list\n",
    "    grams_model = set(terms_model)\n",
    "    temp = 0\n",
    "    for i in grams_reference:\n",
    "        if i in grams_model:\n",
    "            temp = temp + 1\n",
    "    fenmu = len(grams_model) + len(grams_reference) - temp  # 并集\n",
    "    jaccard_coefficient = float(temp / fenmu)  # 交集\n",
    "    return jaccard_coefficient\n",
    "\n",
    "\n",
    "a = \"飞控系统主要组成以及交联设备:飞行传感器系统(子系统)的说明是什么？\"\n",
    "\n",
    "str_input = \"飞控系统的说明是什么\"\n",
    "# jaccard_coefficient = Jaccrad(str_input, b)\n",
    "\n",
    "results = collection.find({})\n",
    "for result in results:\n",
    "    if result['p'] in str_input:\n",
    "        str_split = str_input.split(result['p'])\n",
    "        str_p = result['p']\n",
    "        break\n",
    "str_s = str_split[0]\n",
    "print(str_s)\n",
    "print(str_p)\n",
    "\n",
    "# jaccard_coefficient = Jaccrad(str_s, b)\n",
    "results = collection.find({\"p\":str_p})\n",
    "for result in results:\n",
    "    jacc_score = Jaccrad(str_s,result['s'])\n",
    "    # print(jacc_score)\n",
    "    if jacc_score > 0.1:\n",
    "        print(result['s'],result['p'],result['o'])\n",
    "print(\"--------------------------------------------\")\n",
    "# print(collection.find({\"p\": \"用途\"}).count()) \n",
    "\n",
    "# jaccard_coefficient = Jaccrad(a, b)\n",
    "# print(jaccard_coefficient) //一个系数"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unsigned-circular",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ⭐哈工大LTP-亲测中文处理简单好用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "441ce137",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T14:21:48.840929Z",
     "start_time": "2021-05-04T14:21:33.361643Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "#pip install ltp\n",
    "from ltp import LTP\n",
    "ltp = LTP()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "38b4e773",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T14:32:43.747019Z",
     "start_time": "2021-05-04T14:32:42.216911Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seg:  [['我', '在', '复旦', '大学', '马路边', '看见', '了', '计算机', '学院', '肖仰华', '老师', '。']]\n",
      "pos:  [['r', 'p', 'nz', 'n', 'n', 'v', 'u', 'n', 'n', 'nh', 'n', 'wp']]\n",
      "ner:  [[('Ni', 2, 3), ('Ni', 7, 8), ('Nh', 9, 9)]]\n",
      "srl:  [[[], [], [], [], [], [('A0', 0, 0), ('ARGM-LOC', 1, 4), ('A1', 7, 10)], [], [], [], [], [], []]]\n",
      "dep [[(1, 6, 'SBV'), (2, 6, 'ADV'), (3, 4, 'ATT'), (4, 5, 'ATT'), (5, 2, 'POB'), (6, 0, 'HED'), (7, 6, 'RAD'), (8, 9, 'ATT'), (9, 11, 'ATT'), (10, 11, 'ATT'), (11, 6, 'VOB'), (12, 6, 'WP')]]\n",
      "sdp [[(1, 6, 'AGT'), (2, 5, 'mRELA'), (3, 4, 'FEAT'), (4, 5, 'FEAT'), (5, 6, 'LOC'), (6, 0, 'Root'), (7, 6, 'mDEPD'), (8, 9, 'FEAT'), (9, 11, 'FEAT'), (10, 11, 'FEAT'), (11, 6, 'DATV'), (12, 6, 'mPUNC')]]\n"
     ]
    }
   ],
   "source": [
    "from ltp import LTP\n",
    "\n",
    "ltp = LTP()  # 默认加载 Small 模型\n",
    "seg, hidden = ltp.seg([\"我在复旦大学马路边看见了计算机学院肖仰华老师。\"])\n",
    "print('seg: ',seg)\n",
    "# print(hidden)\n",
    "pos = ltp.pos(hidden)\n",
    "print('pos: ',pos)\n",
    "ner = ltp.ner(hidden)\n",
    "print('ner: ',ner)\n",
    "srl = ltp.srl(hidden)\n",
    "print('srl: ',srl)\n",
    "dep = ltp.dep(hidden)\n",
    "print('dep',dep)\n",
    "sdp = ltp.sdp(hidden)\n",
    "print('sdp',sdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "practical-reputation",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T14:23:15.323728Z",
     "start_time": "2021-05-04T14:23:14.745648Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['他叫汤姆去拿外衣。', '汤姆生病了。', '他去了医院。']\n",
      "[['r', 'v', 'nh', 'v', 'v', 'n', 'wp']]\n",
      "[['我', '在', '复旦', '大学', '马路边', '看见', '了', '计算机', '学院', '肖仰华', '老师', '。']]\n",
      "[[('Ni', 2, 3), ('Ni', 7, 8), ('Nh', 9, 9)]]\n",
      "Ni : 复旦大学\n",
      "[[[], [('A0', 0, 0), ('A1', 2, 2), ('A2', 3, 5)], [], [], [('A0', 2, 2), ('A1', 5, 5)], [], []]]\n",
      "[[(1, [('A0', 0, 0), ('A1', 2, 2), ('A2', 3, 5)]), (4, [('A0', 2, 2), ('A1', 5, 5)])]]\n",
      "[[(1, 2, 'SBV'), (2, 0, 'HED'), (3, 2, 'DBL'), (4, 5, 'ADV'), (5, 2, 'VOB'), (6, 5, 'VOB'), (7, 2, 'WP')]]\n",
      "[[(1, 2, 'AGT'), (2, 0, 'Root'), (3, 2, 'DATV'), (3, 4, 'AGT'), (3, 5, 'AGT'), (4, 2, 'eSUCC'), (5, 2, 'eSUCC'), (5, 4, 'eSUCC'), (6, 5, 'PAT'), (7, 2, 'mPUNC')]]\n"
     ]
    }
   ],
   "source": [
    "#分句\n",
    "sents = ltp.sent_split([\"他叫汤姆去拿外衣。汤姆生病了。他去了医院。\"])\n",
    "print(sents)\n",
    "# user_dict.txt 是词典文件， max_window是最大前向分词窗口,用户自定义词典\n",
    "ltp.init_dict(path=\"user_dict.txt\", max_window=4)\n",
    "# 也可以在代码中添加自定义的词语\n",
    "ltp.add_words(words=[\"负重前行\", \"长江大桥\"], max_window=4)\n",
    "#分词\n",
    "segment,_= ltp.seg([\"他叫汤姆去拿外衣。\"])\n",
    "segment, hidden = ltp.seg([\"他/叫/汤姆/去/拿/外衣/。\".split('/')], is_preseged=True)\n",
    "# print(hidden)\n",
    "#词性标注\n",
    "pos = ltp.pos(hidden)\n",
    "print(pos)\n",
    "# NER\n",
    "seg, hidden = ltp.seg([\"我在复旦大学马路边看见了计算机学院肖仰华老师。\"])\n",
    "print(seg)\n",
    "ner = ltp.ner(hidden)\n",
    "print(ner)\n",
    "# [['他', '叫', '汤姆', '去', '拿', '外衣', '。']]\n",
    "# [[('Nh', 2, 2)]]\n",
    "tag, start, end = ner[0][0]\n",
    "print(tag,\":\", \"\".join(seg[0][start:end + 1]))\n",
    "# Nh : 汤姆\n",
    "# 语义角色标注\n",
    "seg, hidden = ltp.seg([\"他叫汤姆去拿外衣。\"])\n",
    "srl1 = ltp.srl(hidden)\n",
    "print(srl1)\n",
    "srl2 = ltp.srl(hidden, keep_empty=False)\n",
    "print(srl2)\n",
    "# 依存句法分析:在依存句法当中，虚节点ROOT占据了0位置，因此节点的下标从1开始。\n",
    "seg, hidden = ltp.seg([\"他叫汤姆去拿外衣。\"])\n",
    "dep = ltp.dep(hidden)\n",
    "print(dep)\n",
    "# 语义依存分析(树)\n",
    "seg, hidden = ltp.seg([\"他叫汤姆去拿外衣。\"])\n",
    "sdp = ltp.sdp(hidden)\n",
    "print(sdp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "motivated-worry",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T14:34:48.400380Z",
     "start_time": "2021-05-04T14:34:47.055138Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['飞控', '系统', '主要', '组成', '以及', '交联', '设备', ':', '飞行', '传感', '系统', '（', '子系统', '）']]\n",
      "[[]]\n"
     ]
    }
   ],
   "source": [
    "# NER\n",
    "from ltp import LTP\n",
    "ltp = LTP()\n",
    "seg, hidden = ltp.seg([\"飞控系统主要组成以及交联设备:飞行传感系统（子系统）\"])\n",
    "print(seg)\n",
    "ner = ltp.ner(hidden)\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spiritual-houston",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# 甲言-古汉语信息处理"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "occasional-knock",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-01T15:42:03.998819Z",
     "start_time": "2021-04-01T15:42:02.876618Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '庄子.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-f71b56be7541>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mconstructor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPMIEntropyLexiconConstructor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconstruct_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'庄子.txt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mconstructor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlexicon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'庄子词库.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Deep/lib/python3.6/site-packages/jiayan/lexicon/pmi_entropy_constructor.py\u001b[0m in \u001b[0;36mconstruct_lexicon\u001b[0;34m(self, data_file)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mconstruct_lexicon\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_trie_trees\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m         \u001b[0mlexicon\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Deep/lib/python3.6/site-packages/jiayan/lexicon/pmi_entropy_constructor.py\u001b[0m in \u001b[0;36mbuild_trie_trees\u001b[0;34m(self, data_file)\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m         \u001b[0mstart\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mtext\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtext_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m             \u001b[0mlength\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/Deep/lib/python3.6/site-packages/jiayan/utils.py\u001b[0m in \u001b[0;36mtext_iterator\u001b[0;34m(data_file, keep_punc)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mtext_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_punc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;34m\"\"\" A help function to provide clean zh char lines of a given file. \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mseg\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '庄子.txt'"
     ]
    }
   ],
   "source": [
    "from jiayan import PMIEntropyLexiconConstructor\n",
    "\n",
    "constructor = PMIEntropyLexiconConstructor()\n",
    "lexicon = constructor.construct_lexicon('庄子.txt')\n",
    "constructor.save(lexicon, '庄子词库.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "103744a6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ⭐jiagu自然语言处理工具 - 方便快捷 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9558abb",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Jiagu使用大规模语料训练而成。将提供中文分词、词性标注、命名实体识别、情感分析、知识图谱关系抽取、关键词抽取、文本摘要、新词发现、情感分析、文本聚类等常用自然语言处理功能。参考了各大工具优缺点制作，将Jiagu回馈给大家。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "087d2eac",
   "metadata": {
    "hidden": true
   },
   "source": [
    "1. 快速上手：分词、词性标注、命名实体识别"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "introductory-sender",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:04:39.860326Z",
     "start_time": "2021-05-04T04:04:39.838324Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['飞控', '系统', '主要', '组成', '以及', '交', '联', '设备', ':', '飞行', '传感', '系统', '（', '子系统', '）']\n",
      "['n', 'n', 'a', 'v', 'c', 'v', 'v', 'n', 'w', 'v', 'n', 'n', 'w', 'n', 'w']\n",
      "['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "#jiagu.init() # 可手动初始化，也可以动态初始化\n",
    "\n",
    "text = '飞控系统主要组成以及交联设备:飞行传感系统（子系统）'\n",
    "\n",
    "words = jiagu.seg(text) # 分词\n",
    "print(words)\n",
    "\n",
    "pos = jiagu.pos(words) # 词性标注\n",
    "print(pos)\n",
    "\n",
    "ner = jiagu.ner(words) # 命名实体识别\n",
    "print(ner)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4d7610",
   "metadata": {
    "hidden": true
   },
   "source": [
    "2. 中文分词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "72a674b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:07:18.114846Z",
     "start_time": "2021-05-04T04:07:18.098847Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['汉服', '和', '服装', '、', '维基', '图谱']\n",
      "['汉服和服装', '、', '维基', '图谱']\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "text = '汉服和服装、维基图谱'\n",
    "\n",
    "words = jiagu.seg(text)\n",
    "print(words)\n",
    "\n",
    "# jiagu.load_userdict('dict/user.dict') # 加载自定义字典，支持字典路径、字典列表形式。\n",
    "jiagu.load_userdict(['汉服和服装'])\n",
    "\n",
    "words = jiagu.seg(text) # 自定义分词，字典分词模式有效\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eee1d8a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "3. 知识图谱关系抽取\n",
    "\n",
    "仅用于测试用，可以pip3 install jiagu==0.1.8，只能使用百科的描述进行测试。效果更佳的后期将会开放api。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "367edd5c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:27:31.797505Z",
     "start_time": "2021-05-04T04:27:31.760059Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['姚明', '出生日期', '1980年9月12日'], ['姚明', '出生地', '上海市徐汇区'], ['姚明', '祖籍', '江苏省苏州市吴江区震泽镇']]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "# 吻别是由张学友演唱的一首歌曲。\n",
    "# 《盗墓笔记》是2014年欢瑞世纪影视传媒股份有限公司出品的一部网络季播剧，改编自南派三叔所著的同名小说，由郑保瑞和罗永昌联合导演，李易峰、杨洋、唐嫣、刘天佐、张智尧、魏巍等主演。\n",
    "\n",
    "text = '姚明1980年9月12日出生于上海市徐汇区，祖籍江苏省苏州市吴江区震泽镇，前中国职业篮球运动员，司职中锋，现任中职联公司董事长兼总经理。'\n",
    "knowledge = jiagu.knowledge(text)\n",
    "print(knowledge)\n",
    "type(knowledge)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caec18c7",
   "metadata": {
    "hidden": true
   },
   "source": [
    "4. 关键词提取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bdc003b0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:18:04.759244Z",
     "start_time": "2021-05-04T04:18:04.724230Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['.', '工程', '万', '造林', '绿化']\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "text = '''\n",
    "该研究主持者之一、波士顿大学地球与环境科学系博士陈池（音）表示，“尽管中国和印度国土面积仅占全球陆地的9%，但两国为这一绿化过程贡献超过三分之一。考虑到人口过多的国家一般存在对土地过度利用的问题，这个发现令人吃惊。”\n",
    "NASA埃姆斯研究中心的科学家拉玛·内曼尼（Rama Nemani）说，“这一长期数据能让我们深入分析地表绿化背后的影响因素。我们一开始以为，植被增加是由于更多二氧化碳排放，导致气候更加温暖、潮湿，适宜生长。”\n",
    "“MODIS的数据让我们能在非常小的尺度上理解这一现象，我们发现人类活动也作出了贡献。”\n",
    "NASA文章介绍，在中国为全球绿化进程做出的贡献中，有42%来源于植树造林工程，对于减少土壤侵蚀、空气污染与气候变化发挥了作用。\n",
    "据观察者网过往报道，2017年我国全国共完成造林736.2万公顷、森林抚育830.2万公顷。其中，天然林资源保护工程完成造林26万公顷，退耕还林工程完成造林91.2万公顷。京津风沙源治理工程完成造林18.5万公顷。三北及长江流域等重点防护林体系工程完成造林99.1万公顷。完成国家储备林建设任务68万公顷。\n",
    "'''\t\t\t\t\n",
    "\n",
    "keywords = jiagu.keywords(text, 5) # 关键词\n",
    "print(keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a11d49c4",
   "metadata": {
    "hidden": true
   },
   "source": [
    "5. 文本摘要"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "48f3a9ea",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:19:38.879471Z",
     "start_time": "2021-05-04T04:19:38.866472Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['”NASA文章介绍，在中国为全球绿化进程做出的贡献中，有42%来源于植树造林工程，对于减少土壤侵蚀、空气污染与气候变化发挥了作用。']\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "# fin = open('input.txt', 'r')\n",
    "# text = fin.read()\n",
    "# fin.close()\n",
    "\n",
    "text = '''该研究主持者之一、波士顿大学地球与环境科学系博士陈池（音）表示，“尽管中国和印度国土面积仅占全球陆地的9%，但两国为这一绿化过程贡献超过三分之一。考虑到人口过多的国家一般存在对土地过度利用的问题，这个发现令人吃惊。”\n",
    "NASA埃姆斯研究中心的科学家拉玛·内曼尼（Rama Nemani）说，“这一长期数据能让我们深入分析地表绿化背后的影响因素。我们一开始以为，植被增加是由于更多二氧化碳排放，导致气候更加温暖、潮湿，适宜生长。”\n",
    "“MODIS的数据让我们能在非常小的尺度上理解这一现象，我们发现人类活动也作出了贡献。”\n",
    "NASA文章介绍，在中国为全球绿化进程做出的贡献中，有42%来源于植树造林工程，对于减少土壤侵蚀、空气污染与气候变化发挥了作用。\n",
    "据观察者网过往报道，2017年我国全国共完成造林736.2万公顷、森林抚育830.2万公顷。其中，天然林资源保护工程完成造林26万公顷，退耕还林工程完成造林91.2万公顷。京津风沙源治理工程完成造林18.5万公顷。三北及长江流域等重点防护林体系工程完成造林99.1万公顷。完成国家储备林建设任务68万公顷。'''\n",
    "\n",
    "summarize = jiagu.summarize(text, 1) # 摘要\n",
    "print(summarize)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3405ce65",
   "metadata": {
    "hidden": true
   },
   "source": [
    "6. 新词发现"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3c6c764c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:20:18.872495Z",
     "start_time": "2021-05-04T04:20:18.857484Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import jiagu\n",
    "\n",
    "jiagu.findword('input.txt', 'output.txt')\n",
    "# 根据文本，利用信息熵做新词发现。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "318e1d0a",
   "metadata": {
    "hidden": true
   },
   "source": [
    "7. 情感分析"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b42d165e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:20:49.163746Z",
     "start_time": "2021-05-04T04:20:49.140765Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('negative', 0.9957030885091285)\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "text = '很讨厌还是个懒鬼'\n",
    "sentiment = jiagu.sentiment(text)\n",
    "print(sentiment)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59f49784",
   "metadata": {
    "hidden": true
   },
   "source": [
    "8. 文本聚类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f9d32579",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-04T04:21:21.475946Z",
     "start_time": "2021-05-04T04:21:21.458944Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: ['百度深度学习中文情感分析工具Senta试用及在线测试', '深度学习实践：从零开始做电影评论文本情感分析', '将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上', '现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文'], 1: ['自然语言处理工具包spaCy介绍', '情感分析是自然语言处理里面一个热门话题'], 2: ['AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总', 'BERT相关论文、文章和代码资源汇总']}\n"
     ]
    }
   ],
   "source": [
    "import jiagu\n",
    "\n",
    "docs = [\n",
    "        \"百度深度学习中文情感分析工具Senta试用及在线测试\",\n",
    "        \"情感分析是自然语言处理里面一个热门话题\",\n",
    "        \"AI Challenger 2018 文本挖掘类竞赛相关解决方案及代码汇总\",\n",
    "        \"深度学习实践：从零开始做电影评论文本情感分析\",\n",
    "        \"BERT相关论文、文章和代码资源汇总\",\n",
    "        \"将不同长度的句子用BERT预训练模型编码，映射到一个固定长度的向量上\",\n",
    "        \"自然语言处理工具包spaCy介绍\",\n",
    "        \"现在可以快速测试一下spaCy的相关功能，我们以英文数据为例，spaCy目前主要支持英文和德文\"\n",
    "    ]\n",
    "cluster = jiagu.text_cluster(docs)\n",
    "print(cluster)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e045473b",
   "metadata": {
    "hidden": true
   },
   "source": [
    "词性标注说明：\n",
    "\n",
    "n　　　普通名词\n",
    "nt　 　时间名词\n",
    "nd　 　方位名词\n",
    "nl　 　处所名词\n",
    "nh　 　人名\n",
    "nhf　　姓\n",
    "nhs　　名\n",
    "ns　 　地名\n",
    "nn 　　族名\n",
    "ni 　　机构名\n",
    "nz 　　其他专名\n",
    "v　　 动词\n",
    "vd　　趋向动词\n",
    "vl　　联系动词\n",
    "vu　　能愿动词\n",
    "a　 　形容词\n",
    "f　 　区别词\n",
    "m　 　数词　　\n",
    "q　 　量词\n",
    "d　 　副词\n",
    "r　 　代词\n",
    "p　　 介词\n",
    "c　 　连词\n",
    "u　　 助词\n",
    "e　 　叹词\n",
    "o　 　拟声词\n",
    "i　 　习用语\n",
    "j　　 缩略语\n",
    "h　　 前接成分\n",
    "k　　 后接成分\n",
    "g　 　语素字\n",
    "x　 　非语素字\n",
    "w　 　标点符号\n",
    "ws　　非汉字字符串\n",
    "wu　　其他未知的符号"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635f8803",
   "metadata": {
    "hidden": true
   },
   "source": [
    "命名实体说明（采用BIO标记方式）:\n",
    "\n",
    "B-PER、I-PER   人名\n",
    "\n",
    "B-LOC、I-LOC   地名\n",
    "\n",
    "B-ORG、I-ORG   机构名"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "expanded-senior",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# ⭐Synonyms近义词和词向量获取"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "linear-provider",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:54:47.573356Z",
     "start_time": "2021-04-17T12:52:57.469993Z"
    },
    "hidden": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "smart_open library not found; falling back to local-filesystem-only\n",
      "[jieba] default dict file path ..\\data\\vocab.txt\n",
      "[jieba] default dict file path ..\\data\\vocab.txt\n",
      "[jieba] load default dict ..\\data\\vocab.txt ...\n",
      "[jieba] load default dict ..\\data\\vocab.txt ...\n",
      ">> Synonyms load wordseg dict [D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\synonyms\\data\\vocab.txt] ... \n",
      ">> Synonyms on loading stopwords [D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\synonyms\\data\\stopwords.txt] ...\n",
      "[Synonyms] on loading vectors [D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\synonyms\\data\\words.vector.gz] ...\n"
     ]
    }
   ],
   "source": [
    "import synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "complicated-necessity",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-17T12:55:14.920589Z",
     "start_time": "2021-04-17T12:55:14.913585Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "飞控系统:  ([], [])\n"
     ]
    }
   ],
   "source": [
    "print(synonyms.nearby(\"飞控系统\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "manufactured-crisis",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-11T09:02:16.769138Z",
     "start_time": "2021-04-11T09:02:16.753138Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid token (<ipython-input-2-62fd3ea4ada2>, line 9)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-2-62fd3ea4ada2>\"\u001b[1;36m, line \u001b[1;32m9\u001b[0m\n\u001b[1;33m    095, 0.525344, 0.524009, 0.523101, 0.516046])\u001b[0m\n\u001b[1;37m      ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid token\n"
     ]
    }
   ],
   "source": [
    "import synonyms\n",
    "print(\"人脸: \", synonyms.nearby(\"人脸\"))\n",
    "print(\"识别: \", synonyms.nearby(\"识别\"))\n",
    "print(\"NOT_EXIST: \", synonyms.nearby(\"NOT_EXIST\"))\n",
    "\n",
    "synonyms.nearby(人脸, 10) = (\n",
    "    [\"图片\", \"图像\", \"通过观察\", \"数字图像\", \"几何图形\", \"脸部\", \"图象\", \"放大镜\", \"面孔\", \"Mii\"],\n",
    "    [0.597284, 0.580373, 0.568486, 0.535674, 0.531835, 0.530\n",
    "095, 0.525344, 0.524009, 0.523101, 0.516046])\n",
    "#\n",
    "两个句子的相似度比较\n",
    "\n",
    "    sen1 = \"发生历史性变革\"\n",
    "    sen2 = \"发生历史性变革\"\n",
    "    r = synonyms.compare(sen1, sen2, seg=True)\n",
    "    \n",
    "# 以友好的方式打印近义词，方便调试，display(WORD [, SIZE])调用了 synonyms#nearby 方法。\n",
    "\n",
    "synonyms.display(\"飞机\")\n",
    "# '飞机'近义词：\n",
    "#   1. 飞机:1.0\n",
    "#   2. 直升机:0.8423391\n",
    "#   3. 客机:0.8393003\n",
    "#   4. 滑翔机:0.7872388\n",
    "#   5. 军用飞机:0.7832081\n",
    "#   6. 水上飞机:0.77857226\n",
    "#   7. 运输机:0.7724742\n",
    "#   8. 航机:0.7664748\n",
    "#   9. 航空器:0.76592904\n",
    "#   10. 民航机:0.74209654\n",
    "\n",
    "\n",
    "#获得一个词语的向量，该向量为 numpy 的 array，当该词语是未登录词时，抛出 KeyError 异常。\n",
    "synonyms.v(\"飞机\")\n",
    "array([-2.412167  ,  2.2628384 , -7.0214124 ,  3.9381874 ,  0.8219283 ,\n",
    "       -3.2809453 ,  3.8747153 , -5.217062  , -2.2786229 , -1.2572327 ],\n",
    "      dtype=float32)\n",
    "# 中文分词\n",
    "\n",
    "synonyms.seg(\"中文近义词工具包\")\n",
    "# 分词结果，由两个 list 组成的元组，分别是单词和对应的词性。\n",
    "\n",
    "(['中文', '近义词', '工具包'], ['nz', 'n', 'n'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-respect",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "happy-section",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T03:38:56.722740Z",
     "start_time": "2021-04-12T03:38:53.148817Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "准确率: (0, nan, nan)\n"
     ]
    }
   ],
   "source": [
    "# coidng:utf-8\n",
    "import fasttext\n",
    " \n",
    " \n",
    "# 模型的训练\n",
    "def train():\n",
    "    model = fasttext.train_supervised(\"train.txt\", lr=0.1, dim=100, epoch=500, word_ngrams=4, loss='softmax')\n",
    "    model.save_model(\"model_file.bin\")\n",
    " \n",
    " \n",
    "# 模型的测试\n",
    "def test_1():\n",
    "    classifier = fasttext.load_model(\"model_file.bin\")\n",
    "    # 测试模型\n",
    "    result = classifier.test(\"test.txt\")\n",
    "    print(\"准确率:\", result)\n",
    "    f1 = open('prediction.txt', 'w', encoding='utf-8')\n",
    "    with open('test.txt', encoding='utf-8') as fp:\n",
    "        for line in fp.readlines():\n",
    "            line = line.strip()\n",
    "            if line == '':\n",
    "                continue\n",
    "            f1.write(line + '\\t#####\\t' + classifier.predict([line])[0][0][0] + '\\n')\n",
    "    f1.close()\n",
    " \n",
    " \n",
    "if __name__ == '__main__':\n",
    "    train()\n",
    "    test_1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "powered-criterion",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T03:39:22.459759Z",
     "start_time": "2021-04-12T03:39:19.354536Z"
    },
    "hidden": true
   },
   "outputs": [],
   "source": [
    "import fasttext\n",
    "trainDataFile = 'train.txt'\n",
    " \n",
    "classifier = fasttext.train_supervised(\n",
    "    input = trainDataFile,\n",
    "    label_prefix = '__label__',\n",
    "    dim = 256,\n",
    "    epoch = 50,\n",
    "    lr = 1,\n",
    "    lr_update_rate = 50,\n",
    "    min_count = 3,\n",
    "    loss = 'softmax',\n",
    "    word_ngrams = 2,\n",
    "    bucket = 1000000)\n",
    "classifier.save_model(\"Model.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "opponent-speaking",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-04-12T03:40:35.086534Z",
     "start_time": "2021-04-12T03:40:33.547620Z"
    },
    "hidden": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集上数据量 0\n",
      "测试集上准确率 nan\n",
      "测试集上召回率 nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning : `load_model` does not return WordVectorModel or SupervisedModel any more, but a `FastText` object which is very similar.\n"
     ]
    }
   ],
   "source": [
    "testDataFile = 'test.txt'\n",
    " \n",
    "classifier = fasttext.load_model('Model.bin') \n",
    " \n",
    "result = classifier.test(testDataFile)\n",
    "print('测试集上数据量', result[0])\n",
    "print('测试集上准确率', result[1])\n",
    "print('测试集上召回率', result[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7da68e6",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# lightNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba29d558",
   "metadata": {
    "hidden": true
   },
   "source": [
    "是要train的，不是纯工具包\n",
    "使用说明在： https://lightnlp-cookbook.readthedocs.io/zh_CN/latest/preface.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb5b13",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 功能"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e8b2563",
   "metadata": {
    "hidden": true
   },
   "source": [
    "    序列标注，sl\n",
    "        中文分词，cws\n",
    "        命名实体识别，ner\n",
    "        词性标注，pos\n",
    "        语义角色标注， srl\n",
    "    结构分析，sp\n",
    "        基于图的依存句法分析，gdp\n",
    "        基于转移的依存句法分析， tdp\n",
    "    句子关系，sr\n",
    "        语句相似度，ss\n",
    "        文本蕴含，te\n",
    "    文本分类，tc\n",
    "        关系抽取，re\n",
    "        情感极性分析，sa\n",
    "    文本生成，tg\n",
    "        语言模型，lm\n",
    "        聊天机器人，cb\n",
    "        机器翻译，mt\n",
    "        文本摘要，ts\n",
    "    词向量，we\n",
    "        词袋模型，cbow\n",
    "            base\n",
    "            hierarchical_softmax\n",
    "            negative_sampling\n",
    "        跳字模型，skip_gram\n",
    "        base\n",
    "        hierarchical_softmax\n",
    "        negative_sampling\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3851479",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 所用模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e74a9fa",
   "metadata": {
    "hidden": true
   },
   "source": [
    "所用模型：\n",
    "\n",
    "    ner: BiLstm-Crf\n",
    "    cws: BiLstm-Crf\n",
    "    pos: BiLstm-Crf\n",
    "    srl:BiLstm-Crf\n",
    "    sa: TextCnn\n",
    "    re: TextCnn,当前这里只是有监督关系抽取\n",
    "    lm: Lstm,基础的LSTM，没有使用Seq2Seq模型\n",
    "    ss: 共享LSTM + 曼哈顿距离\n",
    "    te:共享LSTM + 全连接\n",
    "    tdp: lstm + mlp + shift-reduce(移入规约)\n",
    "    gdp: lstm + mlp + biaffine（双仿射）\n",
    "    cbow: base、hierarchical_softmax、negative_sampling\n",
    "    skip_gram: base、hierarchical_softmax、negative_sampling\n",
    "    cb: Seq2Seq+Attention\n",
    "    mt: Seq2Seq+Attention\n",
    "    ts: Seq2Seq+Attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8f028c",
   "metadata": {
    "hidden": true
   },
   "source": [
    "## 使用"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212c40a5",
   "metadata": {
    "hidden": true
   },
   "source": [
    "### 关系抽取"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2940e5e8",
   "metadata": {
    "hidden": true
   },
   "source": [
    "训练数据示例如下，其中各列分别为实体1、实体2、关系、句子\n",
    "\n",
    "    钱钟书\t辛笛\t同门\t与辛笛京沪唱和聽钱钟书与钱钟书是清华校友，钱钟书高辛笛两班。\n",
    "    元武\t元华\tunknown\t于师傅在一次京剧表演中，选了元龙（洪金宝）、元楼（元奎）、元彪、成龙、元华、元武、元泰7人担任七小福的主角。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "organizational-serum",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''训练'''\n",
    "from lightnlp.tc import RE\n",
    "\n",
    "re = RE()\n",
    "\n",
    "train_path = '/home/lightsmile/Projects/NLP/ChineseNRE/data/people-relation/train.sample.txt'\n",
    "dev_path = '/home/lightsmile/Projects/NLP/ChineseNRE/data/people-relation/test.sample.txt'\n",
    "vec_path = '/home/lightsmile/NLP/embedding/word/sgns.zhihu.bigram-char'\n",
    "\n",
    "re.train(train_path, dev_path=dev_path, vectors_path=vec_path, save_path='./re_saves')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63952c2b",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''测试'''\n",
    "re.load('./re_saves')\n",
    "re.test(dev_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77ef4bcc",
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "'''预测'''\n",
    "print(re.predict('钱钟书', '辛笛', '与辛笛京沪唱和聽钱钟书与钱钟书是清华校友，钱钟书高辛笛两班。'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f10db787",
   "metadata": {
    "hidden": true
   },
   "source": [
    "预测结果：\n",
    "\n",
    "(0.7306928038597107, '同门') # return格式为（预测概率，预测标签）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f70f602",
   "metadata": {},
   "source": [
    "# fastNLP"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e77909",
   "metadata": {},
   "source": [
    "## 文本分类例程"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b274fc84",
   "metadata": {},
   "source": [
    "### 读取数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40a79f6f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:19:26.230230Z",
     "start_time": "2021-05-05T05:02:34.760175Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  6%|▌         | 98.3k/1.76M [00:00<00:01, 870kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://212.129.155.247/dataset/chn_senti_corp.zip not found in cache, downloading to C:\\Users\\小钒\\AppData\\Local\\Temp\\tmpr02odrxb\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.76M/1.76M [00:00<00:00, 2.34MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download from http://212.129.155.247/dataset/chn_senti_corp.zip\n",
      "Copy file to C:\\Users\\小钒\\.fastNLP\\dataset\\chn_senti_corp\n"
     ]
    }
   ],
   "source": [
    "'''fastNLP提供多种数据的自动下载与自动加载功能，对于这里我们要用到的数据，\n",
    "我们可以用 Loader 自动下载并加载该数据。 更多有关Loader的使用可以参考 loader'''\n",
    "from fastNLP.io import ChnSentiCorpLoader\n",
    "\n",
    "loader = ChnSentiCorpLoader()        # 初始化一个中文情感分类的loader\n",
    "data_dir = loader.download()         # 这一行代码将自动下载数据到默认的缓存地址, 并将该地址返回\n",
    "data_bundle = loader.load(data_dir)  # 这一行代码将从{data_dir}处读取数据至DataBundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6638cb9c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:28:29.519281Z",
     "start_time": "2021-05-05T05:28:29.503280Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 3 datasets:\n",
      "\tdev has 1200 instances.\n",
      "\ttest has 1200 instances.\n",
      "\ttrain has 9600 instances.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''DataBundle的相关介绍，可以参考 DataBundle 。我们可以打印该data_bundle的基本信息。'''\n",
    "print(data_bundle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "61ce0cb5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:33:00.850623Z",
     "start_time": "2021-05-05T05:33:00.842627Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------+--------+\n",
      "| raw_chars                                 | target |\n",
      "+-------------------------------------------+--------+\n",
      "| 选择珠江花园的原因就是方便，有电动扶梯... | 1      |\n",
      "| 15.4寸笔记本的键盘确实爽，基本跟台式机... | 1      |\n",
      "+-------------------------------------------+--------+\n"
     ]
    }
   ],
   "source": [
    "'''可以看出，该data_bundle中一个含有三个 DataSet 。\n",
    "通过下面的代码，我们可以查看DataSet的基本情况'''\n",
    "print(data_bundle.get_dataset('train')[:2])  # 查看Train集前两个sample"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748814e1",
   "metadata": {},
   "source": [
    "### 预处理数据"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d819bbd7",
   "metadata": {},
   "source": [
    "在NLP任务中，预处理一般包括:\n",
    "\n",
    "    将一整句话切分成汉字或者词;\n",
    "    将文本转换为index\n",
    "fastNLP中也提供了多种数据集的处理类，这里我们直接使用fastNLP的ChnSentiCorpPipe。更多关于Pipe的说明可以参考 pipe 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "52ffb556",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:39:32.267599Z",
     "start_time": "2021-05-05T05:39:28.341284Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In total 3 datasets:\n",
      "\tdev has 1200 instances.\n",
      "\ttest has 1200 instances.\n",
      "\ttrain has 9600 instances.\n",
      "In total 2 vocabs:\n",
      "\tchars has 4409 entries.\n",
      "\ttarget has 2 entries.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.io import ChnSentiCorpPipe\n",
    "\n",
    "pipe = ChnSentiCorpPipe()\n",
    "data_bundle = pipe.process(data_bundle)  # 所有的Pipe都实现了process()方法，且输入输出都为DataBundle类型\n",
    "\n",
    "print(data_bundle)  # 打印data_bundle，查看其变化"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b37bfe",
   "metadata": {},
   "source": [
    "可以看到除了之前已经包含的3个 DataSet ,还新增了两个 Vocabulary 。我们可以打印DataSet中的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "27442924",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:40:47.436527Z",
     "start_time": "2021-05-05T05:40:47.414525Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+--------+------------------------+---------+\n",
      "| raw_chars               | target | chars                  | seq_len |\n",
      "+-------------------------+--------+------------------------+---------+\n",
      "| 选择珠江花园的原因就... | 0      | [338, 464, 1400, 78... | 106     |\n",
      "| 15.4寸笔记本的键盘确... | 0      | [50, 133, 20, 135, ... | 56      |\n",
      "+-------------------------+--------+------------------------+---------+\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "fastNLP.io.data_bundle.DataBundle"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(data_bundle.get_dataset('train')[:2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35adfa81",
   "metadata": {},
   "source": [
    "新增了一列为数字列表的chars，以及变为数字的target列。可以看出这两列的名称和刚好与data_bundle中两个Vocabulary的名称是一致的，我们可以打印一下Vocabulary看一下里面的内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4ba6c41",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:43:59.163457Z",
     "start_time": "2021-05-05T05:43:59.153456Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary(['选', '择', '珠', '江', '花']...)\n"
     ]
    }
   ],
   "source": [
    "char_vocab = data_bundle.get_vocab('chars')\n",
    "print(char_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a60e6f77",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T05:44:54.709480Z",
     "start_time": "2021-05-05T05:44:54.700480Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'选'的index是338\n",
      "index:338对应的汉字是选\n"
     ]
    }
   ],
   "source": [
    "'''Vocabulary是一个记录着词语与index之间映射关系的类，比如'''\n",
    "index = char_vocab.to_index('选')\n",
    "print(\"'选'的index是{}\".format(index))  # 这个值与上面打印出来的第一个instance的chars的第一个index是一致的\n",
    "print(\"index:{}对应的汉字是{}\".format(index, char_vocab.to_word(index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b0040fc",
   "metadata": {},
   "source": [
    "### 选择预训练词向量"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f353777",
   "metadata": {},
   "source": [
    "由于Word2vec, Glove, Elmo, Bert等预训练模型可以增强模型的性能，所以在训练具体任务前，选择合适的预训练词向量非常重要。 \n",
    "\n",
    "在fastNLP中我们提供了多种Embedding使得加载这些预训练模型的过程变得更加便捷。 \n",
    "\n",
    "这里我们先给出一个使用word2vec的中文汉字预训练的示例，之后再给出一个使用Bert的文本分类。 这里使用的预训练词向量为'cn-fastnlp-100d'\n",
    "\n",
    "fastNLP将自动下载该embedding至本地缓存， fastNLP支持使用名字指定的Embedding以及相关说明可以参见 fastNLP.embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b98cec4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T06:08:09.901593Z",
     "start_time": "2021-05-05T06:08:07.766451Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 81.9k/3.70M [00:00<00:04, 763kB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://212.129.155.247/embedding/cn_char_fastnlp_100d.zip not found in cache, downloading to C:\\Users\\小钒\\AppData\\Local\\Temp\\tmphek4mbvh\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3.70M/3.70M [00:01<00:00, 2.78MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download from http://212.129.155.247/embedding/cn_char_fastnlp_100d.zip\n",
      "Copy file to C:\\Users\\小钒\\.fastNLP\\embedding\\cn_char_fastnlp_100d\n",
      "Found 4321 out of 4409 words in the pre-training embedding.\n"
     ]
    }
   ],
   "source": [
    "from fastNLP.embeddings import StaticEmbedding\n",
    "word2vec_embed = StaticEmbedding(char_vocab, model_dir_or_name='cn-char-fastnlp-100d')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31ec933d",
   "metadata": {},
   "source": [
    "### 创建模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "431f3625",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T06:17:19.555495Z",
     "start_time": "2021-05-05T06:17:19.523883Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from fastNLP.modules import LSTM\n",
    "import torch\n",
    "\n",
    "# 定义模型\n",
    "class BiLSTMMaxPoolCls(nn.Module):\n",
    "    def __init__(self, embed, num_classes, hidden_size=400, num_layers=1, dropout=0.3):\n",
    "        super().__init__()\n",
    "        self.embed = embed\n",
    "\n",
    "        self.lstm = LSTM(self.embed.embedding_dim, hidden_size=hidden_size//2, num_layers=num_layers,\n",
    "                         batch_first=True, bidirectional=True)\n",
    "        self.dropout_layer = nn.Dropout(dropout)\n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, chars, seq_len):  # 这里的名称必须和DataSet中相应的field对应，比如之前我们DataSet中有chars，这里就必须为chars\n",
    "        # chars:[batch_size, max_len]\n",
    "        # seq_len: [batch_size, ]\n",
    "        chars = self.embed(chars)\n",
    "        outputs, _ = self.lstm(chars, seq_len)\n",
    "        outputs = self.dropout_layer(outputs)\n",
    "        outputs, _ = torch.max(outputs, dim=1)\n",
    "        outputs = self.fc(outputs)\n",
    "\n",
    "        return {'pred':outputs}  # [batch_size,], 返回值必须是dict类型，且预测值的key建议设为pred\n",
    "\n",
    "# 初始化模型\n",
    "model = BiLSTMMaxPoolCls(word2vec_embed, len(data_bundle.get_vocab('target')))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "885f3333",
   "metadata": {},
   "source": [
    "### 训练模型"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c41cbd",
   "metadata": {},
   "source": [
    "fastNLP提供了Trainer对象来组织训练过程，包括完成loss计算(所以在初始化Trainer的时候需要指定loss类型)，梯度更新(所以在初始化Trainer的时候需要提供优化器optimizer)以及在验证集上的性能验证(所以在初始化时需要提供一个Metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0da79395",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-05-05T06:22:15.180842Z",
     "start_time": "2021-05-05T06:19:09.866317Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\tchars: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 106]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2021-05-05-14-19-10-237355\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31997f639f4f4d438cdf0907e7e123b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=3000.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Performance on test is:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=19.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluate data in 10.34 seconds!\n",
      "[tester] \n",
      "AccuracyMetric: acc=0.603333\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'AccuracyMetric': {'acc': 0.603333}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from fastNLP import Trainer\n",
    "from fastNLP import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from fastNLP import AccuracyMetric\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001)\n",
    "metric = AccuracyMetric()\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'  # 如果有gpu的话在gpu上运行，训练速度会更快\n",
    "\n",
    "trainer = Trainer(train_data=data_bundle.get_dataset('train'), model=model, loss=loss,\n",
    "                  optimizer=optimizer, batch_size=32, dev_data=data_bundle.get_dataset('dev'),\n",
    "                  metrics=metric, device=device)\n",
    "trainer.train()  # 开始训练，训练完成之后默认会加载在dev上表现最好的模型\n",
    "\n",
    "# 在测试集上测试一下模型的性能\n",
    "from fastNLP import Tester\n",
    "print(\"Performance on test is:\")\n",
    "tester = Tester(data=data_bundle.get_dataset('test'), model=model, metrics=metric, batch_size=64, device=device)\n",
    "tester.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23de1716",
   "metadata": {
    "ExecuteTime": {
     "start_time": "2021-05-05T06:24:05.173Z"
    },
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0.00/412M [00:00<?, ?B/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "http://212.129.155.247/embedding/bert-chinese-wwm.zip not found in cache, downloading to C:\\Users\\小钒\\AppData\\Local\\Temp\\tmp6f73sd8y\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 412M/412M [04:06<00:00, 1.67MB/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish download from http://212.129.155.247/embedding/bert-chinese-wwm.zip\n",
      "Copy file to C:\\Users\\小钒\\.fastNLP\\embedding\\bert-chinese-wwm\n",
      "loading vocabulary file C:\\Users\\小钒\\.fastNLP\\embedding\\bert-chinese-wwm\\vocab.txt\n",
      "Load pre-trained BERT parameters from file C:\\Users\\小钒\\.fastNLP\\embedding\\bert-chinese-wwm\\chinese_wwm_pytorch.bin.\n",
      "input fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\tchars: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2, 106]) \n",
      "\tseq_len: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "target fields after batch(if batch size is 2):\n",
      "\ttarget: (1)type:torch.Tensor (2)dtype:torch.int64, (3)shape:torch.Size([2]) \n",
      "\n",
      "training epochs started 2021-05-05-14-28-21-762938\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "022db1732c2648c990dcec35d3230bc8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=1800.0), HTML(value='')), layout=Layout(d…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Performance on test is:\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36ca496ee1dc4bb48ac583687acf4aa7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, layout=Layout(flex='2'), max=19.0), HTML(value='')), layout=Layout(dis…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-c9cbd5421797>\", line 30, in <module>\n",
      "    tester.test()\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 175, in test\n",
      "    pred_dict = self._data_forward(self._predict_func, batch_x)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 223, in _data_forward\n",
      "    y = self._predict_func_wrapper(**x)\n",
      "  File \"<ipython-input-11-9938381614ea>\", line 19, in forward\n",
      "    chars = self.embed(chars)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 137, in forward\n",
      "    outputs = self.model(words)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 485, in forward\n",
      "    output_all_encoded_layers=True)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 515, in forward\n",
      "    output_all_encoded_layers=output_all_encoded_layers)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 384, in forward\n",
      "    hidden_states = layer_module(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 362, in forward\n",
      "    attention_output = self.attention(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 320, in forward\n",
      "    self_output = self.self(input_tensor, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 280, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-c9cbd5421797>\", line 30, in <module>\n",
      "    tester.test()\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 175, in test\n",
      "    pred_dict = self._data_forward(self._predict_func, batch_x)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 223, in _data_forward\n",
      "    y = self._predict_func_wrapper(**x)\n",
      "  File \"<ipython-input-11-9938381614ea>\", line 19, in forward\n",
      "    chars = self.embed(chars)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 137, in forward\n",
      "    outputs = self.model(words)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 485, in forward\n",
      "    output_all_encoded_layers=True)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 515, in forward\n",
      "    output_all_encoded_layers=output_all_encoded_layers)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 384, in forward\n",
      "    hidden_states = layer_module(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 362, in forward\n",
      "    attention_output = self.attention(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 320, in forward\n",
      "    self_output = self.self(input_tensor, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 280, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1194, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:Internal Python error in the inspect module.\n",
      "Below is the traceback from this internal error.\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3418, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"<ipython-input-13-c9cbd5421797>\", line 30, in <module>\n",
      "    tester.test()\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 175, in test\n",
      "    pred_dict = self._data_forward(self._predict_func, batch_x)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\core\\tester.py\", line 223, in _data_forward\n",
      "    y = self._predict_func_wrapper(**x)\n",
      "  File \"<ipython-input-11-9938381614ea>\", line 19, in forward\n",
      "    chars = self.embed(chars)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 137, in forward\n",
      "    outputs = self.model(words)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\embeddings\\bert_embedding.py\", line 485, in forward\n",
      "    output_all_encoded_layers=True)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 515, in forward\n",
      "    output_all_encoded_layers=output_all_encoded_layers)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 384, in forward\n",
      "    hidden_states = layer_module(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 362, in forward\n",
      "    attention_output = self.attention(hidden_states, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 320, in forward\n",
      "    self_output = self.self(input_tensor, attention_mask)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\torch\\nn\\modules\\module.py\", line 727, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\fastNLP\\modules\\encoder\\bert.py\", line 280, in forward\n",
      "    attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'KeyboardInterrupt' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3338, in run_ast_nodes\n",
      "    if (await self.run_code(code, result,  async_=asy)):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3435, in run_code\n",
      "    self.showtraceback(running_compiled_code=True)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1194, in structured_traceback\n",
      "    tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2923, in _run_cell\n",
      "    return runner(coro)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\async_helpers.py\", line 68, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3147, in run_cell_async\n",
      "    interactivity=interactivity, compiler=compiler, result=result)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 3357, in run_ast_nodes\n",
      "    self.showtraceback()\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2048, in showtraceback\n",
      "    value, tb, tb_offset=tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1437, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1337, in structured_traceback\n",
      "    self, etype, value, tb, tb_offset, number_of_lines_of_context\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1212, in structured_traceback\n",
      "    chained_exceptions_tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1151, in format_exception_as_a_whole\n",
      "    last_unique, recursion_repeat = find_recursion(orig_etype, evalue, records)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 451, in find_recursion\n",
      "    return len(records), 0\n",
      "TypeError: object of type 'NoneType' has no len()\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\interactiveshell.py\", line 2045, in showtraceback\n",
      "    stb = value._render_traceback_()\n",
      "AttributeError: 'TypeError' object has no attribute '_render_traceback_'\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 1170, in get_records\n",
      "    return _fixed_getinnerframes(etb, number_of_lines_of_context, tb_offset)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 316, in wrapped\n",
      "    return f(*args, **kwargs)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\site-packages\\IPython\\core\\ultratb.py\", line 350, in _fixed_getinnerframes\n",
      "    records = fix_frame_records_filenames(inspect.getinnerframes(etb, context))\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1495, in getinnerframes\n",
      "    frameinfo = (tb.tb_frame,) + getframeinfo(tb, context)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 1453, in getframeinfo\n",
      "    filename = getsourcefile(frame) or getfile(frame)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 696, in getsourcefile\n",
      "    if getattr(getmodule(object, filename), '__loader__', None) is not None:\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 739, in getmodule\n",
      "    f = getabsfile(module)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 708, in getabsfile\n",
      "    _filename = getsourcefile(object) or getfile(object)\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\inspect.py\", line 693, in getsourcefile\n",
      "    if os.path.exists(filename):\n",
      "  File \"D:\\Anaconda\\envs\\Deep\\lib\\genericpath.py\", line 19, in exists\n",
      "    os.stat(path)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "'''使用BERT进行文本分类'''\n",
    "# 只需要切换一下Embedding即可\n",
    "from fastNLP.embeddings import BertEmbedding\n",
    "\n",
    "# 这里为了演示一下效果，所以默认Bert不更新权重\n",
    "bert_embed = BertEmbedding(char_vocab, model_dir_or_name='cn', auto_truncate=True, requires_grad=False)\n",
    "model = BiLSTMMaxPoolCls(bert_embed, len(data_bundle.get_vocab('target')))\n",
    "\n",
    "\n",
    "import torch\n",
    "from fastNLP import Trainer\n",
    "from fastNLP import CrossEntropyLoss\n",
    "from torch.optim import Adam\n",
    "from fastNLP import AccuracyMetric\n",
    "\n",
    "loss = CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=2e-5)\n",
    "metric = AccuracyMetric()\n",
    "device = 0 if torch.cuda.is_available() else 'cpu'  # 如果有gpu的话在gpu上运行，训练速度会更快\n",
    "\n",
    "trainer = Trainer(train_data=data_bundle.get_dataset('train'), model=model, loss=loss,\n",
    "                  optimizer=optimizer, batch_size=16, dev_data=data_bundle.get_dataset('test'),\n",
    "                  metrics=metric, device=device, n_epochs=3)\n",
    "trainer.train()  # 开始训练，训练完成之后默认会加载在dev上表现最好的模型\n",
    "\n",
    "# 在测试集上测试一下模型的性能\n",
    "from fastNLP import Tester\n",
    "print(\"Performance on test is:\")\n",
    "tester = Tester(data=data_bundle.get_dataset('test'), model=model, metrics=metric, batch_size=64, device=device)\n",
    "tester.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:Deep]",
   "language": "python",
   "name": "deep"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
